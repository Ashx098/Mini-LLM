{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Mini-LLM Demo\n",
    "\n",
    "This notebook demonstrates text generation with Mini-LLM, an 80M parameter language model built from scratch with modern LLM architecture components (RoPE, RMSNorm, SwiGLU, GQA).\n",
    "\n",
    "**Model on HuggingFace:** https://huggingface.co/Ashx098/Mini-LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mini-LLM from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Ashx098/Mini-LLM\"\n",
    "\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100, temperature=0.8, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate text using Mini-LLM.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text to start generation\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (higher = more creative)\n",
    "        top_p: Nucleus sampling threshold\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Highlight the generated portion\n",
    "    generated_only = generated_text[len(prompt):]\n",
    "    \n",
    "    return generated_text, generated_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Out!\n",
    "\n",
    "### Example 1: Creative Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, there was a\"\n",
    "full_output, generated = generate_text(prompt, max_new_tokens=80, temperature=0.8)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"def fibonacci(n):\"\n",
    "full_output, generated = generate_text(prompt, max_new_tokens=60, temperature=0.6)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Knowledge Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "full_output, generated = generate_text(prompt, max_new_tokens=30, temperature=0.5)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Story Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In the year 2050, artificial intelligence had\"\n",
    "full_output, generated = generate_text(prompt, max_new_tokens=100, temperature=0.9)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this prompt to whatever you want!\n",
    "custom_prompt = \"The meaning of life is\"\n",
    "\n",
    "full_output, generated = generate_text(custom_prompt, max_new_tokens=80, temperature=0.7)\n",
    "\n",
    "print(f\"Prompt: {custom_prompt}\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Parameters\n",
    "\n",
    "| Parameter | What It Does | Low Value | High Value |\n",
    "|-----------|--------------|-----------|------------|\n",
    "| **temperature** | Controls randomness | More predictable, repetitive | More creative, diverse |\n",
    "| **top_p** | Nucleus sampling threshold | More conservative | More diverse vocabulary |\n",
    "| **max_new_tokens** | Maximum length of generation | Shorter outputs | Longer outputs |\n",
    "\n",
    "**Tips:**\n",
    "- Use `temperature=0.5-0.7` for factual content\n",
    "- Use `temperature=0.8-1.0` for creative writing\n",
    "- Use `temperature=0.2-0.4` for code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Mini-LLM\n",
    "\n",
    "**Mini-LLM** is an 80M parameter decoder-only transformer built from scratch with:\n",
    "\n",
    "- **RoPE** (Rotary Position Embeddings) - Better sequence extrapolation\n",
    "- **RMSNorm** - Faster, more stable normalization\n",
    "- **SwiGLU** - State-of-the-art activation function\n",
    "- **GQA** (Grouped Query Attention) - Efficient attention mechanism\n",
    "- **SentencePiece BPE** - Real-world tokenization with 32K vocabulary\n",
    "\n",
    "**GitHub:** https://github.com/yourusername/Mini-LLM  \n",
    "**HuggingFace:** https://huggingface.co/Ashx098/Mini-LLM\n",
    "\n",
    "---\n",
    "\n",
    "*Built with ‚ù§Ô∏è by MSR Avinash*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}